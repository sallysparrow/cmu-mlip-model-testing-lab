{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "# Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "API key must be 40 characters long, yours was 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# wandb.login(key = \"\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#replace with your key\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cmu-mlip-model-testing-lab/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_login.py:80\u001b[39m, in \u001b[36mlogin\u001b[39m\u001b[34m(anonymous, key, relogin, host, force, timeout, verify, referrer)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Set up W&B login credentials.\u001b[39;00m\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m \u001b[33;03mBy default, this will only store credentials locally without\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m    UsageError: If `api_key` cannot be configured and no tty.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m _handle_host_wandb_setting(host)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[43m=\u001b[49m\u001b[43manonymous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cmu-mlip-model-testing-lab/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_login.py:325\u001b[39m, in \u001b[36m_login\u001b[39m\u001b[34m(anonymous, key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent, _disable_warning)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_is_pre_configured:\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m update_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m         \u001b[43mwlogin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtry_save_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     wlogin.update_session(key, status=key_status)\n\u001b[32m    327\u001b[39m     wlogin._update_global_anonymous_setting()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cmu-mlip-model-testing-lab/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_login.py:181\u001b[39m, in \u001b[36m_WandbLogin.try_save_api_key\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m         \u001b[43mapikey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m apikey.WriteNetrcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    183\u001b[39m         wandb.termwarn(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cmu-mlip-model-testing-lab/.venv/lib/python3.12/site-packages/wandb/sdk/lib/apikey.py:313\u001b[39m, in \u001b[36mwrite_key\u001b[39m\u001b[34m(settings, key, api)\u001b[39m\n\u001b[32m    310\u001b[39m _, suffix = key.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, key)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(suffix) != \u001b[32m40\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI key must be 40 characters long, yours was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    315\u001b[39m write_netrc(settings.base_url, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, key)\n",
      "\u001b[31mValueError\u001b[39m: API key must be 40 characters long, yours was 2"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.login(key = \"\")\n",
    "wandb.login(key=\"ca7aad83edf119127bfff120288b123954d34330\") #replace with your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2025\"    \n",
    "ENTITY = None                        \n",
    "RUN_NAME = \"tweet_eval_roberta_vs_gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"roberta\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"gpt2\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label normalization \n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "HF_LABEL_MAP = {\"LABEL_0\":\"negative\",\"LABEL_1\":\"neutral\",\"LABEL_2\":\"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True   # set False to use tweets.csv fallback\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: “I had a blue penis while I was this” [pla...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  @user @user what do these '1/2 naked pics' hav...  neutral\n",
       "1  OH: “I had a blue penis while I was this” [pla...  neutral\n",
       "2  @user @user That's coming, but I think the vic...  neutral"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "# Step 3 - Add MetaData for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "In this step, you'll add **5 metadata columns** to your dataset to enable slicing later in **Weights & Biases (W&B)**.\n",
    "\n",
    "You can use:\n",
    "- **Value matching** (e.g., tweets with hashtags)\n",
    "- **Regex** (e.g., strong positive words like *love*, *great*)\n",
    "- **Heuristics** (e.g., emoji count, all-caps detection, tweet length)\n",
    "\n",
    "These columns will be carried forward when you run inference in Step 6 and will appear in your final `predictions_table` logged to W&B.\n",
    "\n",
    "---\n",
    "\n",
    "Once inference is complete, your W&B table (`df_long`) will include:\n",
    "- Original tweet text\n",
    "- Ground-truth labels\n",
    "- Model predictions and confidence scores\n",
    "- All slicing metadata you define here\n",
    "\n",
    "Later, in the W&B UI, you can use the ➕ `Filter` option in the table view to explore model behavior across these slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 – Add Slicing Metadata\n",
    "# Add new columns for filtering in W&B later\n",
    "\n",
    "# Example: count emojis in each tweet & create a slice for tweets with >3 emojis\n",
    "def count_emojis(text):\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "\n",
    "def get_slices(df):\n",
    "    return {\n",
    "        \"emoji_gt3\": df[\"emoji_count\"] > 3,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0\n",
      "transformers: 4.56.1\n",
      "CUDA available: False\n",
      "Python: /opt/anaconda3/envs/aieng/bin/python\n"
     ]
    }
   ],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "#  Step 4 – Run Inference on Tweets Using Two Sentiment Models\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest:   0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest: 100%|██████████| 500/500 [00:15<00:00, 31.66it/s]\n",
      "Device set to use cpu\n",
      "Infer: LYTinn/finetuning-sentiment-model-tweet-gpt2: 100%|██████████| 500/500 [00:10<00:00, 47.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.804726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: “I had a blue penis while I was this” [pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.866949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.763724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.774047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.416398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  emoji_count  \\\n",
       "0  @user @user what do these '1/2 naked pics' hav...   neutral            0   \n",
       "1  OH: “I had a blue penis while I was this” [pla...   neutral            0   \n",
       "2  @user @user That's coming, but I think the vic...   neutral            0   \n",
       "3  I think I may be finally in with the in crowd ...  positive            0   \n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...  negative            0   \n",
       "\n",
       "     model      pred      conf  \n",
       "0  roberta  negative  0.804726  \n",
       "1  roberta   neutral  0.866949  \n",
       "2  roberta   neutral  0.763724  \n",
       "3  roberta  positive  0.774047  \n",
       "4  roberta   neutral  0.416398  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id, texts):\n",
    "    clf = pipeline(\"text-classification\", model=model_id, truncation=True, framework=\"pt\", device=-1)\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, df[\"text\"].tolist())\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"]  = yhat\n",
    "    tmp[\"conf\"]  = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "# Step 5: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qf/pctqvp2j307dcm_wq781bf_h0000gn/T/ipykernel_53214/1664234989.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]))\n"
     ]
    }
   ],
   "source": [
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = list(y_true)\n",
    "    y_pred = list(y_pred)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "overall = (\n",
    "    df_long.groupby(\"model\")\n",
    "           .apply(lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]))\n",
    ")\n",
    "\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}  # Initialize inner dict\n",
    "\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = compute_accuracy(g[\"label\"], g[\"pred\"])\n",
    "        acc = float(acc) \n",
    "        # Add to wandb Table\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        # Add to dict\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6: Log to Wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nikitac/Documents/CMU-Fall2025/MLIP-TA/cmu-mlip-model-testing-lab/wandb/run-20250917_040100-pcdt9jzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025/runs/pcdt9jzl' target=\"_blank\">tweet_eval_roberta_vs_gpt2</a></strong> to <a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025/runs/pcdt9jzl' target=\"_blank\">https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025/runs/pcdt9jzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpt2_accuracy</td><td>0.398</td></tr><tr><td>roberta_accuracy</td><td>0.698</td></tr><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>1</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025/runs/pcdt9jzl' target=\"_blank\">https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025/runs/pcdt9jzl</a><br> View project at: <a href='https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/nikitac-cmu/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250917_040100-pcdt9jzl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME, config={\n",
    "    \"models\": MODELS,\n",
    "    \"n_rows\": len(df),\n",
    "    \"use_hf_dataset\": USE_HF_DATASET\n",
    "})\n",
    "\n",
    "# Main predictions table: one row per (example, model)\n",
    "pred_table = wandb.Table(dataframe=df_long)\n",
    "wandb.log({\"predictions_table\": pred_table})\n",
    "\n",
    "# Log overall accuracy to wandb summary\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "\n",
    "# wandb.log({\"slice_accuracy_table\": slice_table})\n",
    "for slice_name, model_dict in slice_metrics.items():\n",
    "    for model_name, acc in model_dict.items():\n",
    "        metric_name = f\"slice/{slice_name}/{model_name}_accuracy\"\n",
    "        wandb.log({metric_name: acc})\n",
    "\n",
    "\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "## Instructions: Exploring Slice-Based Evaluation in W&B\n",
    " Step 1: Open the W&B Project\n",
    "- Click on the **project link** above.\n",
    "- Click on the **latest run** near the top.\n",
    "Step 2: View Tables\n",
    "- Click the **\"Tables\"** tab.\n",
    "- You should see:\n",
    "  - `predictions_table`\n",
    "  - `slice_metrics`\n",
    "\n",
    "Step 3: Use Filters in `predictions_table`\n",
    "- Click on `predictions_table`.\n",
    "- Use the filter bar to explore:\n",
    "Example (see image):\n",
    "  ```python\n",
    "  col2 == 0\n",
    "Step 4: \n",
    "- Check slice_metrics table: It shows accuracy of each model for every slice.\n",
    "- Add a Bar Chart Panel: Click the \"Add panels\" button (top-right).\n",
    "- Choose Bar chart under \"Charts\".\n",
    "Try to create bar charts comparing accuracies of both models for a slice. Do it for 2 slices.\n",
    "\n",
    "Discuss your findings with your TA.\n",
    "\n",
    "# Filtering: \n",
    "<img src=\"images/filtering.png\" alt=\"Predictions Table\" width=\"600\">\n",
    "\n",
    "## Plotting:\n",
    "<img src=\"images/plotting.png\" alt=\"Predictions Table\" height=\"300\">\n",
    "<img src=\"images/bar-charts.png\" alt=\"Predictions Table\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students: replace the placeholders below with 1–2 sentence insights\n",
    "saved_slice_notes = [\"\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3039bf2",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices & view them in Wandb.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. [YOUR CHOICE]\n",
    "4. [YOUR CHOICE]\n",
    "5. [YOUR CHOICE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these three slices & re-run the notebook to see them on Wandb.\n",
    "\n",
    "additional_slice_ideas = [\n",
    "\"\"\n",
    "]\n",
    "additional_slice_ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e58835",
   "metadata": {},
   "source": [
    "# Step 7 - Write down three addition data slices you want to create but do not have the metadata for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2b21d",
   "metadata": {},
   "source": [
    "In the previous step, you might have already come up with some slices you wanted to create but found it hard to do with existing metadata. Write down three of such slices in this step.\n",
    "\n",
    "Example: \n",
    "- I want to create a slice on tweets using slangs\n",
    "- I want to create a slice on non-English tweets (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da135c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down three additional data slices here:\n",
    "\n",
    "additional_slice_descriptions = [\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "# Step 8 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That’s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your 10 generated tweets here:\n",
    "generated_slice_description = \"\"\n",
    "\n",
    "generated_cases = [\"\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
