{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "# Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a071bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.login(key = \"\")\n",
    "wandb.login(key=\"ca7aad83edf119127bfff120288b123954d34330\") #replace with your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2025\"    \n",
    "ENTITY = None                        \n",
    "RUN_NAME = \"tweet_eval_roberta_vs_gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"roberta\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"gpt2\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label normalization \n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "HF_LABEL_MAP = {\"LABEL_0\":\"negative\",\"LABEL_1\":\"neutral\",\"LABEL_2\":\"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True   # set False to use tweets.csv fallback\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: “I had a blue penis while I was this” [pla...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  @user @user what do these '1/2 naked pics' hav...  neutral\n",
       "1  OH: “I had a blue penis while I was this” [pla...  neutral\n",
       "2  @user @user That's coming, but I think the vic...  neutral"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "# Step 3 - Add MetaData for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "In this step, you'll add **5 metadata columns** to your dataset to enable slicing later in **Weights & Biases (W&B)**.\n",
    "\n",
    "You can use:\n",
    "- **Value matching** (e.g., tweets with hashtags)\n",
    "- **Regex** (e.g., strong positive words like *love*, *great*)\n",
    "- **Heuristics** (e.g., emoji count, all-caps detection, tweet length)\n",
    "\n",
    "These columns will be carried forward when you run inference in Step 6 and will appear in your final `predictions_table` logged to W&B.\n",
    "\n",
    "---\n",
    "\n",
    "Once inference is complete, your W&B table (`df_long`) will include:\n",
    "- Original tweet text\n",
    "- Ground-truth labels\n",
    "- Model predictions and confidence scores\n",
    "- All slicing metadata you define here\n",
    "\n",
    "Later, in the W&B UI, you can use the ➕ `Filter` option in the table view to explore model behavior across these slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 – Add Slicing Metadata\n",
    "# Add new columns for filtering in W&B later\n",
    "\n",
    "# Example: count emojis in each tweet & create a slice for tweets with >3 emojis\n",
    "def count_emojis(text):\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "\n",
    "def get_slices(df):\n",
    "    return {\n",
    "        \"emoji_gt3\": df[\"emoji_count\"] > 3,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a54cbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Add Slicing Metadata (5 new columns) ---\n",
    "\n",
    "# Regex patterns for slicing\n",
    "HASHTAG_RE    = re.compile(r'(?<!\\w)#\\w+', re.UNICODE)  # start or whitespace before '#'\n",
    "MENTION_RE    = re.compile(r'(?<!\\w)@\\w+', re.UNICODE)\n",
    "URL_RE        = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "STRONG_POS_RE = re.compile(r'\\b(?:love|great|awesome|fantastic|amazing|best|excellent|wonderful|terrific)\\b',\n",
    "                           re.IGNORECASE)\n",
    "\n",
    "df[\"has_hashtag\"]    = df[\"text\"].str.contains(HASHTAG_RE, na=False)\n",
    "df[\"has_mention\"]    = df[\"text\"].str.contains(MENTION_RE, na=False)\n",
    "df[\"url_count\"]      = df[\"text\"].str.findall(URL_RE).str.len().astype(int)\n",
    "df[\"strong_pos_lex\"] = df[\"text\"].str.contains(STRONG_POS_RE, na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cpu\n",
      "transformers: 4.56.1\n",
      "CUDA available: False\n",
      "Python: /home/cat/cmu-mlip-model-testing-lab/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "#  Step 4 – Run Inference on Tweets Using Two Sentiment Models\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77609fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF: 4.56.1 is_torch_available(): True\n"
     ]
    }
   ],
   "source": [
    "import transformers, transformers.pipelines as _pp\n",
    "from transformers.utils import is_torch_available\n",
    "print(\"HF:\", transformers.__version__, \"is_torch_available():\", is_torch_available())\n",
    "_pp.torch = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>url_count</th>\n",
       "      <th>strong_pos_lex</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roberta</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.804726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: “I had a blue penis while I was this” [pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.866949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.763724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roberta</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.774047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.416397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  emoji_count  \\\n",
       "0  @user @user what do these '1/2 naked pics' hav...   neutral            0   \n",
       "1  OH: “I had a blue penis while I was this” [pla...   neutral            0   \n",
       "2  @user @user That's coming, but I think the vic...   neutral            0   \n",
       "3  I think I may be finally in with the in crowd ...  positive            0   \n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...  negative            0   \n",
       "\n",
       "   has_hashtag  has_mention  url_count  strong_pos_lex    model      pred  \\\n",
       "0        False         True          0           False  roberta  negative   \n",
       "1        False        False          0           False  roberta   neutral   \n",
       "2        False         True          0           False  roberta   neutral   \n",
       "3         True         True          0           False  roberta  positive   \n",
       "4        False         True          0           False  roberta   neutral   \n",
       "\n",
       "       conf  \n",
       "0  0.804726  \n",
       "1  0.866949  \n",
       "2  0.763724  \n",
       "3  0.774047  \n",
       "4  0.416397  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = -1  # CPU\n",
    "def run_pipeline(model_id, texts):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    clf = pipeline(\"text-classification\", model=mdl, tokenizer=tok, truncation=True, device=device)\n",
    "    preds, confs = [], []\n",
    "    for t in texts:\n",
    "        out = clf(t)[0]\n",
    "        preds.append(HF_LABEL_MAP.get(out[\"label\"], out[\"label\"]))\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "\n",
    "pred_frames = []\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, df[\"text\"].tolist())\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"]  = yhat\n",
    "    tmp[\"conf\"]  = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "df_long.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "# Step 5: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = list(y_true)\n",
    "    y_pred = list(y_pred)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "overall = (\n",
    "    df_long.groupby(\"model\")[[\"label\", \"pred\"]]\n",
    "           .apply(lambda g: accuracy_score(g[\"label\"], g[\"pred\"]))\n",
    "           .rename(\"accuracy\")\n",
    ")\n",
    "\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}  # Initialize inner dict\n",
    "\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = compute_accuracy(g[\"label\"], g[\"pred\"])\n",
    "        acc = float(acc) \n",
    "        # Add to wandb Table\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        # Add to dict\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6: Log to Wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>overall/gpt2_accuracy</td><td>▁</td></tr><tr><td>overall/roberta_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>overall/gpt2_accuracy</td><td>0.398</td></tr><tr><td>overall/roberta_accuracy</td><td>0.698</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/to1vy70o' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/to1vy70o</a><br> View project at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250919_031034-to1vy70o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cat/cmu-mlip-model-testing-lab/wandb/run-20250919_110555-8rsx6f39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/8rsx6f39' target=\"_blank\">tweet_eval_roberta_vs_gpt2</a></strong> to <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/8rsx6f39' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/8rsx6f39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpt2_accuracy</td><td>0.398</td></tr><tr><td>roberta_accuracy</td><td>0.698</td></tr><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>1</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/8rsx6f39' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/8rsx6f39</a><br> View project at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250919_110555-8rsx6f39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME, config={\n",
    "    \"models\": MODELS,\n",
    "    \"n_rows\": len(df),\n",
    "    \"use_hf_dataset\": USE_HF_DATASET\n",
    "})\n",
    "\n",
    "# Main predictions table: one row per (example, model)\n",
    "pred_table = wandb.Table(dataframe=df_long)\n",
    "wandb.log({\"predictions_table\": pred_table})\n",
    "\n",
    "# Log overall accuracy to wandb summary\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "\n",
    "# wandb.log({\"slice_accuracy_table\": slice_table})\n",
    "for slice_name, model_dict in slice_metrics.items():\n",
    "    for model_name, acc in model_dict.items():\n",
    "        metric_name = f\"slice/{slice_name}/{model_name}_accuracy\"\n",
    "        wandb.log({metric_name: acc})\n",
    "\n",
    "\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "## Instructions: Exploring Slice-Based Evaluation in W&B\n",
    " Step 1: Open the W&B Project\n",
    "- Click on the **project link** above.\n",
    "- Click on the **latest run** near the top.\n",
    "Step 2: View Tables\n",
    "- Click the **\"Tables\"** tab.\n",
    "- You should see:\n",
    "  - `predictions_table`\n",
    "  - `slice_metrics`\n",
    "\n",
    "Step 3: Use Filters in `predictions_table`\n",
    "- Click on `predictions_table`.\n",
    "- Use the filter bar to explore:\n",
    "Example (see image):\n",
    "  ```python\n",
    "  col2 == 0\n",
    "Step 4: \n",
    "- Check slice_metrics table: It shows accuracy of each model for every slice.\n",
    "- Add a Bar Chart Panel: Click the \"Add panels\" button (top-right).\n",
    "- Choose Bar chart under \"Charts\".\n",
    "Try to create bar charts comparing accuracies of both models for a slice. Do it for 2 slices.\n",
    "\n",
    "Discuss your findings with your TA.\n",
    "\n",
    "# Filtering: \n",
    "<img src=\"images/filtering.png\" alt=\"Predictions Table\" width=\"600\">\n",
    "\n",
    "## Plotting:\n",
    "<img src=\"images/plotting.png\" alt=\"Predictions Table\" height=\"300\">\n",
    "<img src=\"images/bar-charts.png\" alt=\"Predictions Table\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0\n",
       "0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students: replace the placeholders below with 1–2 sentence insights\n",
    "saved_slice_notes = [\"\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3039bf2",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices & view them in Wandb.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. has_url — tweets containing at least one URL\n",
    "4. negation_present — tweets with explicit negation (hard for sentiment models)\n",
    "5. short_text_le20 — very short tweets (≤20 tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3345162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these three slices & re-run the notebook to see them on Wandb.\n",
    "\n",
    "additional_slice_ideas = [\n",
    "\"\"\n",
    "]\n",
    "additional_slice_ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metadata needed (add if not already present) ---\n",
    "# URL presence\n",
    "df[\"url_count\"] = df[\"text\"].str.findall(re.compile(r'(?:https?://\\S+|www\\.\\S+)', re.I)).str.len().astype(int)\n",
    "df[\"has_url\"] = df[\"url_count\"] > 0\n",
    "\n",
    "# Negation presence\n",
    "NEGATION_RE = re.compile(\n",
    "    r\"\\b(?:no|not|never|without|n't|cannot|can't|won't|don't|didn't|isn't|aren't|\"\n",
    "    r\"couldn't|shouldn't|wasn't|weren't)\\b\", re.I\n",
    ")\n",
    "df[\"negation_present\"] = df[\"text\"].str.contains(NEGATION_RE, na=False)\n",
    "\n",
    "# Short text (by tokens)\n",
    "df[\"token_len\"] = df[\"text\"].astype(str).str.split().str.len().astype(int)\n",
    "df[\"short_text_le20\"] = df[\"token_len\"] <= 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7acb6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ensure required metadata exist on df\n",
    "# URL presence\n",
    "if \"url_count\" not in df.columns:\n",
    "    URL_RE = re.compile(r'(?:https?://\\S+|www\\.\\S+)', re.I)\n",
    "    df[\"url_count\"] = df[\"text\"].astype(str).str.findall(URL_RE).str.len().astype(int)\n",
    "df[\"has_url\"] = df[\"url_count\"] > 0\n",
    "\n",
    "# Negation presence\n",
    "NEGATION_RE = re.compile(\n",
    "    r\"\\b(?:no|not|never|without|n't|cannot|can't|won't|don't|didn't|isn't|aren't|\"\n",
    "    r\"couldn't|shouldn't|wasn't|weren't)\\b\",\n",
    "    re.I\n",
    ")\n",
    "df[\"negation_present\"] = df[\"text\"].astype(str).str.contains(NEGATION_RE, na=False)\n",
    "\n",
    "# Length features\n",
    "df[\"token_len\"] = df[\"text\"].astype(str).str.split().str.len().astype(int)\n",
    "df[\"short_text_le20\"] = df[\"token_len\"] <= 20\n",
    "\n",
    "# 2) Propagate metadata columns from df -> df_long using modulo index alignment\n",
    "meta_cols = [\"has_url\", \"negation_present\", \"token_len\", \"short_text_le20\"]\n",
    "n_base = len(df)\n",
    "idx_map = (np.arange(len(df_long)) % n_base)\n",
    "\n",
    "for col in meta_cols:\n",
    "    df_long[col] = df[col].values[idx_map]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d92fb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpt2_accuracy</td><td>0.398</td></tr><tr><td>roberta_accuracy</td><td>0.698</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/ssfun8ig' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/ssfun8ig</a><br> View project at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250919_113414-ssfun8ig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cat/cmu-mlip-model-testing-lab/wandb/run-20250919_113711-5ro9cc7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/5ro9cc7b' target=\"_blank\">tweet_eval_roberta_vs_gpt2</a></strong> to <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/5ro9cc7b' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/5ro9cc7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "/tmp/ipykernel_39249/1548506242.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  sub = frame[mask]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>slice/has_url/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/has_url/roberta_accuracy</td><td>▁</td></tr><tr><td>slice/has_url_complement/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/has_url_complement/roberta_accuracy</td><td>▁</td></tr><tr><td>slice/negation_present/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/negation_present/roberta_accuracy</td><td>▁</td></tr><tr><td>slice/negation_present_complement/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/negation_present_complement/roberta_accuracy</td><td>▁</td></tr><tr><td>slice/short_text_le20/gpt2_accuracy</td><td>▁</td></tr><tr><td>slice/short_text_le20/roberta_accuracy</td><td>▁</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpt2_accuracy</td><td>0.398</td></tr><tr><td>roberta_accuracy</td><td>0.698</td></tr><tr><td>slice/has_url/gpt2_accuracy</td><td>0.5625</td></tr><tr><td>slice/has_url/roberta_accuracy</td><td>0.5</td></tr><tr><td>slice/has_url_complement/gpt2_accuracy</td><td>0.39256</td></tr><tr><td>slice/has_url_complement/roberta_accuracy</td><td>0.70455</td></tr><tr><td>slice/negation_present/gpt2_accuracy</td><td>0.25641</td></tr><tr><td>slice/negation_present/roberta_accuracy</td><td>0.67949</td></tr><tr><td>slice/negation_present_complement/gpt2_accuracy</td><td>0.42417</td></tr><tr><td>slice/negation_present_complement/roberta_accuracy</td><td>0.70142</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/5ro9cc7b' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025/runs/5ro9cc7b</a><br> View project at: <a href='https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/canranw-carnegie-mellon-university/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 3 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250919_113711-5ro9cc7b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Slice vs. unslice metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def acc_on_mask(frame, mask):\n",
    "    sub = frame[mask]\n",
    "    if len(sub) == 0:\n",
    "        return None\n",
    "    return float(accuracy_score(sub[\"label\"], sub[\"pred\"]))\n",
    "\n",
    "# Optional: overall per-model\n",
    "overall = (\n",
    "    df_long.groupby(\"model\")[[\"label\",\"pred\"]]\n",
    "           .apply(lambda g: (g[\"label\"] == g[\"pred\"]).mean())\n",
    "           .rename(\"accuracy\")\n",
    ")\n",
    "\n",
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME, reinit=True)\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.run.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "\n",
    "SLICES = {\n",
    "    \"has_url\":           df_long[\"has_url\"],\n",
    "    \"negation_present\":  df_long[\"negation_present\"],\n",
    "    \"short_text_le20\":   df_long[\"short_text_le20\"],\n",
    "}\n",
    "\n",
    "for slice_name, mask in SLICES.items():\n",
    "    tbl = wandb.Table(columns=[\"group\", \"model\", \"accuracy\", \"n_rows\"])\n",
    "    for model_name, g in df_long.groupby(\"model\"):\n",
    "        acc_slice = acc_on_mask(g, mask)\n",
    "        acc_unslc = acc_on_mask(g, ~mask)\n",
    "        tbl.add_data(f\"{slice_name}=True\",  model_name, acc_slice,  int(mask.sum()))\n",
    "        tbl.add_data(f\"{slice_name}=False\", model_name, acc_unslc, int((~mask).sum()))\n",
    "        if acc_slice is not None:\n",
    "            wandb.log({f\"slice/{slice_name}/{model_name}_accuracy\": acc_slice})\n",
    "        if acc_unslc is not None:\n",
    "            wandb.log({f\"slice/{slice_name}_complement/{model_name}_accuracy\": acc_unslc})\n",
    "    wandb.log({f\"{slice_name}_accuracy\": tbl})\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e58835",
   "metadata": {},
   "source": [
    "# Step 7 - Write down three addition data slices you want to create but do not have the metadata for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2b21d",
   "metadata": {},
   "source": [
    "In the previous step, you might have already come up with some slices you wanted to create but found it hard to do with existing metadata. Write down three of such slices in this step.\n",
    "\n",
    "Example: \n",
    "- I want to create a slice on tweets using slangs\n",
    "- I want to create a slice on non-English tweets (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da135c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down three additional data slices here:\n",
    "\n",
    "additional_slice_descriptions = [\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff5a14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_slice_descriptions = [\n",
    "    \"has_url: tweets containing at least one URL (url_count > 0)\",\n",
    "    \"negation_present: tweets with explicit negation terms (e.g., not, never, can't)\",\n",
    "    \"short_text_le20: tweets with token_len ≤ 20\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "# Step 8 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That’s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your 10 generated tweets here:\n",
    "generated_slice_description = \"\"\n",
    "\n",
    "generated_cases = [\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4c2a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_slice_description = \"negation_present — tweets containing explicit negation tokens (e.g., no, not, never, can’t, won’t, don’t, didn’t, isn’t, ain’t, without). Mix average, boundary, and difficult cases (polarity flips, litotes, sarcasm), with varied lengths and occasional hashtags/@mentions/URLs.\"\n",
    "\n",
    "generated_cases = [\n",
    "    \"Not gonna lie, this rollout isn’t the disaster people said.\",\n",
    "    \"Can’t say it’s great, but it’s not terrible either.\",\n",
    "    \"No way this isn’t broken for EU users, @support.\",\n",
    "    \"I’m not unhappy with the update—just not convinced it fixes the lag.\",\n",
    "    \"Won’t pretend the price hike helps; it doesn’t.\",\n",
    "    \"Didn’t hate the beta, didn’t love it; not sure I’d recommend it yet.\",\n",
    "    \"Without sounding dramatic, the sync still isn’t reliable. #productivity\",\n",
    "    \"Never thought I’d say this, but it’s not bad (for once): https://ex.am/ple\",\n",
    "    \"Ain’t no way the ‘instant’ checkout isn’t timing out again, @brand.\",\n",
    "    \"It’s not that I’m mad, I just can’t use this for work given the bugs.\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
